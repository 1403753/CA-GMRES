%\documentclass{report}
\documentclass{scrartcl}
\setlength{\parindent}{0in}
\makeatletter
\DeclareOldFontCommand{\rm}{\normalfont\rmfamily}{\mathrm}
\DeclareOldFontCommand{\sf}{\normalfont\sffamily}{\mathsf}
\DeclareOldFontCommand{\tt}{\normalfont\ttfamily}{\mathtt}
\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
\DeclareOldFontCommand{\it}{\normalfont\itshape}{\mathit}
\DeclareOldFontCommand{\sl}{\normalfont\slshape}{\@nomath\sl}
\DeclareOldFontCommand{\sc}{\normalfont\scshape}{\@nomath\sc}
\makeatother

\usepackage{multirow}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\numberwithin{equation}{section}
\usepackage{amssymb}
\usepackage{lscape}
\usepackage{abstract}

\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage[dvipsnames]{xcolor}
\usepackage{yfonts}
\usepackage[T1]{fontenc}
\usepackage[toc,page]{appendix}


\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usepackage{blkarray}
\usepackage{cancel}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{color}
\usepackage{pdfpages}
\usepackage{helvet}
\usepackage{tikz}
\usetikzlibrary{graphs}


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{babyblueeyes}{rgb}{0.63, 0.79, 0.95}
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}

\lstdefinestyle{base}{
  inputencoding=latin10,
  emptylines=1,
  breaklines=true,
  basicstyle=\small\ttfamily,
  moredelim=**[is][\color{red}]{@}{@},
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

%% Define a HUGE 
\makeatletter
\newcommand\HUGE{\@setfontsize\Huge{24.88}{50}}
\makeatother

\begin{document}             % End of preamble and beginning of text.
\pagenumbering{Roman}
 
%titlepage
\thispagestyle{empty}
\begin{center}
\begin{minipage}{.9\linewidth}
\flushright
	      		 
	%University logo
    \includegraphics[width=0.5\linewidth]{univie.eps}\par
\centering 	
    % Title
	{\scshape{\HUGE Bachelorarbeit\par}}
	\vspace{1cm}
	%Thesis title
    {\scshape{\Large Implementation and experimental comparison between the COMMUNICATION AVOIDING-GENERALIZED MINIMAL RESIDUAL METHOD and standard GMRES \par}}
    \vspace{2cm}
    
  
 Verfasser  \linebreak
 {\Large Robert Ernstbrunner \par}
 	\vspace{.7cm}
angestrebter akademischer Grad\linebreak
 {\Large Bachelor of Science (BSc)\par}
	\vspace{.7cm}

\flushleft
	

\begin{tabular}{ll}
Wien, 2019	\linebreak
\vspace{.5cm}&   \\
  Studienkennzahl lt. Studienblatt: & A 033 521 \vspace{.3cm} \\ 
  Fachrichtung: & Informatik  - Scientific Computing
  \vspace{.3cm} \\
  Betreuerin / Betreuer: & Univ.-Prof. Dipl.-Ing. Dr.\\ 
  & Wilfried Gansterer, M.Sc. \\
 \end{tabular}
 

\end{minipage}
\end{center}
\clearpage

\pagebreak

\tableofcontents

\pagebreak

\section*{Notation}
Similar notation is considered as in Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} and Grigori et al. \cite{Grigori}.
\subsection*{Linear Algebra}
\begin{itemize}
\item Greek letters denote scalars, lower case Roman letters denote vectors (or - based on the context - dimensions), capital Roman letters denote matrices.
\item Capital letters with two subscripts, e.g. '$V_{m, n}$', denote matrices with $m$ rows and $n$ columns.
\item Capital \textit{Black letter} letters (e.g. $V$, $Q$ and $R$ in Black letters are represented by $\mathfrak{V}$, $\mathfrak{Q}$ and $\mathfrak{R}$ resp.) denote matrices that are composed out of other matrices. 
\item $v_k$ denotes the $k^{th}$ vector in a series of vectors $v_0$, $v_1$, $\ldots$, ,  $v_k$, $v_{k+1}$, $\ldots$ of equal length.
\item Similarly, $V_k$ denotes the $k^{th}$ matrix in a sequence of matrices $V_0$, $V_1$, $\ldots$ , $V_k$, $V_{k+1}$, $\ldots$. Generally all these matrices have the same number of rows. They may or may not have the same number of columns.
\item If $V$ is a matrix consisting of $s$ vectors $\left[v_1, v_2, \ldots, v_s\right]$, then $\underline{V} = \left[V, v_{s+1} \right]$. The underline denotes one more column at the end.
\item If again, $V$ is a matrix consisting of $s$ vectors $\left[v_1, v_2, \ldots, v_s\right]$, then $\acute{V} = [v_2, v_3, \ldots, v_s]$. The acute denotes one column less at the beginning.
\item As a consequence $\underline{\acute{V}}$ denotes one more column at the end and one less column at the beginning, e.g. $\underline{\acute{V}} = [\acute{V}, v_{s+1}]$.
\item Depending on the context, both underline or/and acute letters can also refer to rows as well. 
\item $0_{m, n}$ is defined as an $m \times n$ matrix consisting of zeros, $I_{m}$ denotes the $m \times m$ Identity matrix and $e_k$ denotes the $k^{\text{th}}$ canonical vector with the dimension depending on the context.
\item All matrices and vectors are assumed to be real, if not stated otherwise.
\item Matlab notation is used for addressing elements of matrices and vectors. For example, given a matrix $A$ of size $n \times n$ and two sets of indices $\alpha$ and $\beta$, $A(\alpha,:)$ is a submatrix formed by the subset of the rows of $A$ whose indices belong to $\alpha$. Similarly, $A(\alpha, \beta)$ is a submatrix formed by the subset of the rows of $A$ whose indices belong to $\alpha$ and the subset of the columns of $A$ whose indices belong to $\beta$.
\end{itemize}

\subsection*{Terms and definitions}
\begin{itemize}
\item \textbf{Fetching}: the movement of data. This could either be \textit{reading}, \textit{copying} or \textit{sending} and \textit{receiving} messages.
\item \textbf{Ghosting}: the storage of redundant data that does not belong to a processors assigned domain.
\end{itemize}

\subsection*{Graph Notation}
\begin{itemize}
\item $G(A)$ denotes the directed graph of $A$ with $G(A) = \{V,E\}$ where $V(G(A))$ is the set of vertices of $G(A)$ and $E(G(A))$ is the set of edges of $G(A)$.
\item $R(G(A), \alpha)$ denotes the set of vertices in $G(A)$ that are reachable from any vertex in the set $\alpha$, including $\alpha$.
\item $R(G(A), \alpha, m)$ denotes the set of vertices in $G(A)$ that are reachable by paths of length at most $m$ from any vertex in $\alpha$, including $\alpha$.
\end{itemize}

\subsection*{Abbreviations}
\begin{itemize}
\item \textbf{MPK}: Matrix powers kernel
\item \textbf{TSQR}: Tall and skinny QR factorization
\item \textbf{CGS}: Classical Gram-Schmidt method
\item \textbf{MGS}: Modified Gram-Schmidt method
\item \textbf{BCGS}: Block Classical Gram-Schmidt method
\item \textbf{MKL}: Intel Math Kernel Library
\item \textbf{SPD}: symmetric positive definite
\end{itemize}
\pagebreak

\pagenumbering{arabic}
\begin{onecolabstract}
In this thesis the communication avoiding GMRES method named CA-GMRES of Hoemmen et al. [\textit{Communication-avoiding Krylov Subspace Methods. PhD thesis}, Berkeley, CA, USA, 2010] is examined. CA-GMRES is a Krylov Subspace method based on so called $s$-step methods and enables the solving of large sparse (nonsymmetric) linear systems.\\ Its main advantage over known standard GMRES implementations is the reduction of communication by a factor $s$ both sequentially and in parallel, while at the same time being able to converge at the same rate as standard GMRES. It does so by allowing the basis and the restart length to be separately chosen, which adds numerical stability and improves convergence compared to other $s$-step methods. This often leads to significant performance gains.\\ The CA-GMRES method was implemented in a shared-memory environment and then compared to a standard parallel version. Moreover, the ILU(0) preconditioner was incorporated and its behavior was analyzed in order to produce additional numerical and performance results that further support the work and findings of Hoemmen et al..
\end{onecolabstract}

\section{Introduction}
The costs of an algorithm consist of arithmetic computations and communication.
Compared to arithmetic, communication costs are much higher and the widening CPU-memory performance gap and increased parallelism in hardware environments further necessitate the construction of communication-avoiding algorithms.\\
The term \textit{communication} generally denotes the movement of data either between different processors in the parallel case or between 'fast' and 'slow' memory in the sequential case, where 'fast' and 'slow' are relative to the two levels examined in the memory hierarchy (e.g., cache and DRAM, or DRAM and disk). Communication optimal algorithms do not eliminate communication completely, but they are constructed in a way that prioritizes the reduction of communication. This often results in new challenges. E.g., in the case of CA-GMRES (Section~\ref{sec:ca-gmres}), the data dependencies between the sparse matrix-vector multiplies and dot products in the Arnoldi process are broken up, which further necessitates the incorporation of additional techniques and algorithms in order to deal with numerical instabilities and ill-conditioned basis vectors.\\

Krylov subspace methods are iterative algorithms that can be used for solving large, sparse linear systems either in real or complex arithmetic. Many scientific computations spend much of their time in Krylov methods. Therefore, it only makes sense to try to improve these methods by reducing their communication cost. Krylov methods can be 'one-sided' or 'two-sided', have long recurrences or short recurrences, and may work for specific matrix types only (e.g., the popular Conjugate Gradient method is one-sided, has short recurrences and works for symmetric matrices only). For a more detailed description of Krylov properties see Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638}. The Generalized minimal residual method (GMRES) is a one-sided Krylov subspace method that uses long recurrences and works for general matrices. The goal of the communication-avoiding version, namely CA-GMRES, is to take $s$ steps for the same communication cost as 1 step, which would be optimal.\\

The rest of this thesis is organized as follows. The second section introduces GMRES and the basic principles behind the GMRES method. The third section explains the most important kernels in the communication-avoiding version of the GMRES method. Section four gives insight into the $s$-step based Arnoldi process of Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638}. Section five presents the communication-avoiding GMRES method, explains how a preconditioner can be applied and addresses implementation details. Finally, sections six and seven show results for the numerical resp. performance experiments and section eight summarizes and concludes on these results.
 
\section{GMRES}
The Generalized Minimal Residual Method (GMRES) was first introduced by Saad et al. \cite{Saad:1986:GGM:14063.14074} and is an iterative Krylov subspace method for solving large sparse linear systems. 
The GMRES method starts with an initial approximate solution $x_0$ and initial residual $r_0 = b - Ax_0$ and finds a correction $z_k$ at iteration $k$ which solves the least-squares problem 
\begin{equation}
	z_k := \text{argmin}_{z} \norm{b - A(x_0 + z)}_2
\end{equation}
where $z_k$ is determined in the Krylov subspace 
\begin{equation*}
	 \mathcal{K}_k(A, r_0) = \text{span}\{r_0, Ar_0, \ldots, A^{k-1}r_0\}.
\end{equation*}
The solution at iteration $k$ is then formed by $x_k = x_0 + z_k$.
Since $\{r_0, Ar_0, \ldots, A^{k-1}r_0\}$ is usually ill-conditioned the Arnoldi method is incorporated to produce $k + 1$ orthonormal basis vectors $\underline{Q} = [q_1, q_2, \ldots, q_k, q_{k + 1}]$ with $q_1 = r_0/\norm{r_0}_2$ and a $k + 1 \times k$ upper Hessenberg coefficient matrix $\underline{H}$ where
\begin{equation*}
	AQ = \underline{Q}\underline{H}.
\end{equation*}
With these conditions $z_k$ can be defined as $z := Qy$ such that 
\begin{eqnarray*}
	\text{argmin}_{z} \norm{b - A(x_0 + z)}_2 &=& \text{argmin}_y \norm{r_0 - AQy}_2 \\
	 &=& \text{argmin}_{y} \lVert r_0 - \underline{Q} \underline{H} y \rVert_2.
\end{eqnarray*}
Since $q_1 = r_0/\norm{r_0}_2$ and $\underline{Q}$ is orthonormal, one has
\begin{eqnarray} \label{eq:stdgmreslsp}
	\text{argmin}_y \lVert r_0 - \underline{Q} \underline{H} y \rVert_2 &=& \text{argmin}_{y} \lVert \underline{Q}^T r_0 - \underline{H} y \rVert_2 \nonumber \\
	&=& \text{argmin}_{y} \norm{\beta e_1 - \underline{H} y}_2 
\end{eqnarray}
with $\beta = \norm{r_0}_2$.
$\underline{H}$ is then factored into $\underline{H} = \underline{G}\underline{U}$ with square matrix $\underline{G}$ being a product of $k$ Givens rotations, $\underline{U} = 
\begin{pmatrix}
	U \\
	0_{1, k}
\end{pmatrix}$
and $U$ being upper triangular. Also, $\underline{g}$ is defined by $\underline{g} := \beta \underline{G}^T e_1 = \begin{pmatrix}
g\\
\gamma
\end{pmatrix}$,
where g is a subvector and $\gamma$ is the last entry in $\underline{g}$. The triangular system to solve is then given by
\begin{equation*}
	y_k := \text{argmin}_y \norm{g - U y}_2
\end{equation*}
The solution at iteration $k$ is obtained by computing $x_k = x_0 + Qy_k$. Note, that the absolute value of $\gamma$ is $\norm{b - Ax_k}_2$, the absolute residual at iteration $k$.


\section{Computational kernels}
%introduction; definitions: kernel, communication-avoiding
In this thesis \textit{computational kernels} define parts of an algorithm with significantly high costs, relatively speaking. These costs include both arithmetic operations and communication. The following kernels make up the essential building blocks in Arnoldi($s,t$) (see Section~\ref{sec:ca_arnoldi}) and eventually CA-GMRES (see Section~\ref{sec:ca-gmres}).


\subsection{Matrix powers kernel}
The matrix powers kernel, as described by Hoemmen et al. in \cite{Hoemmen:2010:CKS:1970638}, was not implemented in the context of this thesis. However, it is an essential part to avoid communication and therefore, will be briefly summarized here.\\
In its basic form, the MPK takes an $n \times n$ matrix $A$ and a starting vector $v_1$ as input and produces $s$ more vectors $Av, A^2v, \ldots, A^sv$. Since $A$ is usually large and sparse, it makes sense to look at the graph of $A$ as well, namely $G(A)$ in order to apply known graph algorithms. In $s$-step methods, the MPK replaces the sparse matrix-vector products that generate the basis for the Krylov subspace $\mathcal{K}_{s + 1}(A, v) = \left[v, Av, A^2v, \ldots, A^{s}v\right] = \left[v_1, v_2, \ldots, v_{s + 1}\right]$. One invocation of the MPK produces the same amount of basis vectors as $s$ sparse matrix-vector products. The MPK sends a factor of $\Theta(s)$ fewer messages than $s$ SpMV invocations and the matrix has to be read from slow to fast memory only once.
In order to achieve this, the data and the workload are distributed among $P$ processors, where each processor is assigned a part $\alpha$ of $A(\alpha,:)$ and $v_1(\alpha)$ with $\alpha \subseteq V(G(A))$. Then, each processor fetches $A(\eta,:)$ and $v_1(\eta)$, with $\eta = R(G(A), \alpha, s) - \alpha$ in order to compute $s$ more vectors $v_2(\alpha), v_3(\alpha), \ldots, v_{s + 1}(\alpha)$ without communication. In other words, to compute $v_2(\alpha)$, the superset $\beta = R(G(A),\alpha, 1)$ is required. To compute $v_3(\alpha)$, the set $R(G(A),\beta, 1) = R(G(A),\alpha, 2)$ must be available. In general, to compute $v_{s + 1}(\alpha)$, the set $R(G(A),\alpha, s)$ must be at hand. Since
\begin{equation*}
\alpha \subseteq R(G(A),\alpha, 1) \subseteq \ldots \subseteq R(G(A),\alpha, s - 1) \subseteq R(G(A),\alpha, s)
\end{equation*} it is clear that larger steps eventually lead to increasing amounts of ghosted data and floating point operations.

\subsubsection{Preconditioned matrix powers kernel}
Iterative Krylov methods often require a preconditioner that, when applied, fundamentally changes the MPK. In order to avoid communication, highly parallelizable preconditioners come to mind. E.g., Nuentsa et al. \cite{nuen11c} present their parallel GMRES with a multiplicative Schwarz preconditioner. Grigori et al. \cite{Grigori} developed CA-ILU(0), a very interesting type of preconditioner that, at first glance, seems unfit for a parallel and communication-avoiding environment. Section~\ref{sec:ca-ilu} summarizes their work.

\iffalse
\cite{Hoemmen:2010:CKS:1970638} p.60\\
The Matrix Powers Kernel 
Power iteration, SpMV instead of MV, sparse matrix like a graph $\rightarrow$ spacial, temporal locality not as efficiently used as in dense MV. 
Avoid communication by sending / receiving all necessary values beforehand (look at reachability of graph(A)) and computing s basis vectors without further communication.\\
\texttt{To minimize communication in a parallel setting, the s Monomial basis vectors of the Krylov subspace $[y, Ay, A2y, \ldots, A^sy]$ are computed with no communication using the so-called matrix powers kernel [13]. This requires ghosting and computing redundantly on each processor the data required for computing its part of the vectors with no communication. Note that throughout this paper we use the term ghosting to denote the storage of redundant data, of vectors or matrices, that do not belong to the processor's assigned domain or part, but are needed for future computations.\\
First, the data and the work is split between $P$ processors. Each processor is assigned a part $\alpha$ of the input vector $y_0$ ($y_0(\alpha)$) and $A(\alpha, :)$, where $\alpha \subseteq V(G(A))$. Then, each processor has to compute the same part $\alpha$ of $y_1 = Ay_0$, $y_2 = Ay_1$, till $y_s = Ay_{s - 1}$ without communicating with other processors. To do so, each 
processor fetches all the data needed from the neighboring processors, to compute its part $\alpha$ of the $s$ vectors. Thus, to compute $y_s(\alpha)$, each processor should receive the missing data of $y_0(\eta_s)$ and $A(\eta_s, :)$ from its neighboring processors and store it redundantly, where $\eta_s = R(G(A), \alpha, s)$. Finally, each processor computes the set $R(G(A), \alpha, s - i)$ of the vectors $y_i$ for $i = 1, 2, \ldots, s$ without any communication with the other processors.}
\fi

\subsection{Tall and skinny QR} \label{sec:tsqr}
TSQR is a QR decomposition algorithm especially suited for $m \times n$ matrices, where $m \gg n$. TSQR uses a divide-and-conquer approach and, therefore, works on a reduction tree structure. The highest form of parallelism is achieved if TSQR uses a binary tree. In the purely sequential case a linear (flat) tree comes into play. Hybrid algorithms use anything in between and the best tree structure may depend on the matrix size and underlying architecture as Demmel et al. explain in~\cite{Demmel:2012:CA-QR_demmel}. The parallel TSQR with a binary tree is summarized below. For a more detailed description, as well as a description of the sequential algorithm, see Demmel et al.~\cite{Demmel:2012:CA-QR_demmel}.
\paragraph{Parallel TSQR} First, the matrix $A$ is split up into $P$ parts with each submatrix having size $m/P \times n$. TSQR on a binary tree then passes $(\log_2P) + 1$ stages where any fast and accurate QR factorization can be applied for each stage. Let's assume $P = 4$, then 
\begin{equation*}
A =
\begin{pmatrix}
A_0\\A_1\\A_2\\A_3
\end{pmatrix}.
\end{equation*}
At stage zero the QR factorization for each submatrix $A_i$ is computed, with
\begin{equation*}
A_0 = Q_{00}R_{00}, \qquad A_1 = Q_{10}R_{10}, \qquad A_2 = Q_{20}R_{20} \quad \text{and} \quad A_3 = Q_{30}R_{30}.
\end{equation*}
The successive stage merges the $R$-factors and computes the next QR factorizations
\begin{equation*}
\begin{pmatrix}
R_{00} \\
R_{10}
\end{pmatrix} =
Q_{01}R_{01} \qquad \text{and} \qquad
\begin{pmatrix}
R_{20} \\
R_{30}
\end{pmatrix} = 
Q_{11}R_{11}.
\end{equation*}
This procedure is repeated until one last QR factorization is performed where the final $R$ factor can be interpreted as the root of the tree. This results in the following decomposition
\begin{equation} \label{eq:tsqr}
A =
\begin{pmatrix}
A_0\\A_1\\A_2\\A_3
\end{pmatrix} =
\left(
	\begin{array}{c|c|c|c}
	Q_{00} & & & \\
	\hline
	& Q_{10} & & \\
	\hline
	& & Q_{20} & \\
	\hline
	& & & Q_{30}
	\end{array}
	\right) \cdot
\left(
	\begin{array}{c|c}
		Q_{01} & \\
		\hline
		& Q_{11} 
	\end{array}
\right) \cdot
Q_{02} \cdot R_{02}.
\end{equation}

Figure~\ref{fig:tsqr} shows that this approach only requires $\mathcal{O}(\log_2 P)$ messages on $P$ processors (a factor of $\Theta (s)$ fewer messages than Householder QR or MGS). \cite{Demmel:2012:CA-QR_demmel} show that, sequentially, the matrix is read only once, saving a factor of $\Theta(s)$ transferred data between levels of the memory hierarchy (compared to Householder QR or MGS). 
Of course,~\eqref{eq:tsqr} is stored implicitly to save storage space.\\
The orthogonality of the $Q$ factor computed by Classical or Modified Gram-Schmidt depends on the condition number of $A$. Householder QR, on the other hand, does not make any assumptions on $\mathcal{K}(A)$, it is \textit{unconditionally} stable. Therefore, Householder QR is a good choice for the local QR factorizations in TSQR, which makes TSQR inherently unconditionally stable as well.\\

For reasons explained in Section~\ref{sec:scaling_first_basis_vec} one might want TSQR to produce an $R$ factor with real nonnegative diagonal entries. Demmel et al. show in $\cite{doi:10.1137/nonnegdiags_demmel} $ how to modify the usual Householder QR in a numerically stable way so that such a factor is generated. This modified Householder QR factorization can then be incorporated in order to let TSQR as well produce an $R$ factor with real nonnegative diagonal entries.


\begin{figure}
	\centering
\tikz [new set = A,
	   new set = Q,
	   new set = QQ,
	   new set = R,
	   new set = RR
	   ] {

\node [set = A, rectangle, minimum width=0.7cm, fill=babyblueeyes] at (1,3) {$A_0$};
\node [set = A, rectangle, minimum width=0.7cm, fill=babyblueeyes] at (1,2) {$A_1$};
\node [set = A, rectangle, minimum width=0.7cm, fill=babyblueeyes] at (1,1) {$A_2$};
\node [set = A, rectangle, minimum width=0.7cm, fill=babyblueeyes] at (1,0) {$A_3$};
\node (Q00) [set = Q] at (2.5,3) {$Q_{00}$};
\node (Q10) [set = Q] at (2.5,2) {$Q_{10}$};
\node (Q20) [set = Q] at (2.5,1) {$Q_{20}$};
\node (Q30) [set = Q] at (2.5,0) {$Q_{30}$};
\node [set = R, rectangle,
		minimum height=1.6cm,
		minimum width=0.7cm,
		align=center,
		fill=babyblueeyes] at (3.2,2.5) {};
\node [set = R, rectangle,
		minimum height=1.6cm,
		minimum width=0.7cm,
		align=center,
		fill=babyblueeyes] at (3.2,0.5) {};

\node (Q01) [set = QQ] at (4.7, 2.5) {$Q_{01}$};

\node (Q11) [set = QQ] at (4.7, 0.5) {$Q_{11}$};

\node [set = RR, rectangle,
		minimum height=2.6cm,
		minimum width=0.7cm,
		align=center,
		fill=babyblueeyes] at (5.4, 1.5) {};

\node (Q02R02) [set = QQ] at (7.2, 1.5) {$Q_{02}R_{02}$};

\node (R01) at (5.4, 2.5) {$R_{01}$};
\node (R11) at (5.4, 0.5) {$R_{11}$};
\node (R00) at (3.2, 3) {$R_{00}$};
\node (R10) at (3.2, 2) {$R_{10}$};
\node (R20) at (3.2, 1) {$R_{20}$};
\node (R30) at (3.2, 0) {$R_{30}$};

\graph { 
	(A) ->[matching] (Q),
	(R) ->[matching] (QQ),
	(RR) ->[matching] (Q02R02)
	};
}
\caption{Parallel TSQR on a binary tree of four processors. The first subscript of the $Q$ and $R$ matrices indicates the sequence number for a stage and the second subscript is the stage number. The blue boxes represent the processors involved at each stage.}
\label{fig:tsqr}
\end{figure}

\subsection{Block Gram-Schmidt} \label{sec:block gram-schmidt}
The Gram-Schmidt process takes a set of $s$ linearly independent basis vectors $V = [v_1, \ldots, v_s]$ and creates an orthonormal basis that spans the same subspace as $V$. Unlike unblocked Gram-Schmidt methods, blocked Gram-Schmidt algorithms work on blocks of columns at a time instead of one column at a time. If the matrix consists of $s$ columns, this usually requires a factor of $\Theta (s)$ fewer messages and a factor of $\Theta(s)$ fewer data transfers between levels of the memory hierarchy.\\
In their performance analysis with a simplified parallel model, Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} also show that blocked classical Gram-Schmidt is superior to a blocked modified Gram-Schmidt variant in terms of the messages sent between processors. However, they state that BCGS and BMGS contain similar accuracy properties as their unblocked versions. Therefore, BCGS is not as numerically stable as the blocked MGS variant. The modified Gram-Schmidt method is often used for basis orthogonalization in the Arnoldi iteration (Section~\ref{sec:ca_arnoldi}). Greenbaum et al. showed in \cite{Greenbaum97numericalbehaviour}, that it is the linear independence of the Arnoldi basis, not the orthogonality near machine precision, that is important when solving linear systems with GMRES. Therefore, the CA-GMRES algorithm (Section~\ref{sec:ca-gmres}) can make use of the classical Gram-Schmidt approach that is presented below.

\paragraph{BCGS with TSQR} Algorithm~\ref{alg:bcgs} shows a version of BCGS that incorporates TSQR in order to improve vector orthogonalization. BCGS orthogonalizes the $s$ basis vectors in $V_k$ against all previous basis vectors in $Q$ by computing
\begin{equation}
	V_k' := (I - QQ^T)V_k = V_k - Q(Q^T V_k).
\end{equation}

TSQR then orthogonalizes the $s$ basis vectors with respect to each other. Combined, these two kernels do the work of updating a QR factorization with new columns. The advantage of TSQR and BCGS over unblocked MGS is that they move asymptotically less data between levels of the memory hierarchy. Unlike MGS, BCGS consists almost entirely of dense matrix-matrix operations. Also, TSQR improves orthogonality of the block columns. Therefore, if Algorithm~\ref{alg:bcgs} is used for solving linear systems, reorthogonalization can be omitted entirely.\\
Since TSQR stores its $Q$ matrices implicitly, they must be extracted for the highly optimized dense matrix-matrix operations of BCGS. This step involves additional communication but should be still faster than directly performing the local QR-like computations that are required for the implicit representation.

\begin{algorithm}
\caption{BCGS with TSQR}
\label{alg:bcgs}
\begin{algorithmic}[1]
    \REQUIRE $V = \left[V_1, V_2, \ldots, V_M\right]$ where $V$ is $n \times m$. Each $V_k$ is $n \times m_k$, with $\sum^M_{k=1}m_k = m$.
    \ENSURE \mbox{$Q = \left[Q_1, \ldots, Q_M\right]$, where $\mathcal{R}(Q) = \mathcal{R}(V)$ and $\mathcal{R}(\left[Q_1, \ldots, Q_k \right]) = \mathcal{R}(\left[V_1, \ldots, V_k \right])$}
    \ENSURE $R: m \times m$ upper triangular matrix.
    \FOR{k = 1 to $M$}
    	\STATE $R_{1:k-1,k} := [Q_1, \ldots, Q_{k - 1}]^TV_k$
    	\STATE $V'_k := V_k - [Q_1, \ldots, Q_{k - 1}]R_{1:k - 1, k}$
    	\STATE Compute $V'_k = Q_kR_{kk}$ via TSQR
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\iffalse
\begin{algorithm}[H]
\caption{BMGS with TSQR}
\label{alg:bmgs}
\begin{algorithmic}[1]
    \REQUIRE $V = \left[V_1, V_2, \ldots, V_M\right]$ where $V$ is $n \times m$. Each $V_k$ is $n \times m_k$, with $\sum^M_{k=1}m_k = m$.
    \ENSURE \mbox{$Q = \left[Q_1, \ldots, Q_M\right]$, where $\mathcal{R}(Q) = \mathcal{R}(V)$ and $\mathcal{R}(\left[Q_1, \ldots, Q_k \right]) = \mathcal{R}(\left[V_1, \ldots, V_k \right])$}
    \ENSURE $R: m \times m$ upper triangular matrix.
    \FOR{k = 1 to $M$}
    	\STATE $V_{k}^{(1)} := V_k$
    	\FOR{j = 1 to k - 1}
    		\STATE $R_{jk} := Q_j^T V_k^{(j)}$
	    	\STATE $V_k^{(j + 1)} := V_k^{(j)} - Q_j R_{jk}$
	    \ENDFOR
	    \STATE $V_k' := V_k^{(k)}$
	    \STATE Compute $V'_k = Q_kR_{kk}$ via TSQR
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\fi
\section{CA-Arnoldi}\label{sec:ca_arnoldi}
\subsection{Arnoldi Iteration}

The Arnoldi iteration is a method for solving sparse nonsymmetric eigenvalue problems and was first introduced by W. Arnoldi in \cite{arnoldi:hal-01712943}. $S$ steps of standard Arnoldi produce an $s + 1 \times s$ upper Hessenberg matrix $\underline{H}$ and $m \times s + 1$ orthonormal vectors $\underline{Q} = [q_1$, $q_2$, $\ldots$, $q_s$, $q_{s + 1}]$, where 
\begin{equation} \label{eq:AQ=QH}
	AQ = \underline{Q} \underline{H}.
\end{equation}
In the GMRES method the columns of $\underline{Q}$ form a basis for the Krylov Subspace $\mathcal{K}_{s + 1}(A, r_0)$. There are many ways to orthogonalize successive basis vectors. Modified Gram-Schmidt is often employed because it performs numerically better compared to classical Gram-Schmidt (MGS based Arnoldi is outlined in Algorithm~\ref{alg:mgs_arnoldi}). On the other hand, CGS is more suited for parallel implementations, because it provides fewer synchronization points.
Walker \cite{doi:10.1137/Walker} used Householder QR instead because it provides better orthogonalization than MGS. Since Householder QR requires the vectors to be available all at once, Walker produced $s$ Monomial basis vectors first. Bai et al. \cite{doi:10.1093/imanum/NewtonGMRES_bai} later improved on Walkers work by replacing the (usually ill-conditioned) Monomial basis with the Newton basis. Greenbaum et al. \cite{Greenbaum97numericalbehaviour} later showed, that the loss of orthogonality caused by MGS usually does not affect the solution of the linear system at all. This gave rise to a new communication avoiding version of the Arnoldi method that will be used by CA-GMRES, namely Arnoldi($s,t$).

\begin{algorithm}[H]
\caption{MGS based Arnoldi iteration}
\label{alg:mgs_arnoldi}
\begin{algorithmic}[1]
    \REQUIRE $n \times n$ matrix $A$ and  starting vector $v$ of size $n$
	\ENSURE Orthonormal $n \times s + 1$ matrix $\underline{Q} = [Q,q_{s + 1}]$, and a nonsingular $s + 1 \times s$ upper Hessenberg matrix $\underline{H}$ such that $AQ = \underline{Q} \underline{H}$
    \STATE $\beta := \norm{v}_2$, $q_1 := v/\beta$ 
   	\FOR{$j = 1$ to $s$}
		\STATE $w_j := Aq_j$
		\FOR{$i = 1$ to $j$}
			\STATE $h_{ij} :=  \left< w, q_i \right>$
			\STATE $w_j := w_j - h_{ij}q_i$
		\ENDFOR
		\STATE $h_{j + 1, j} := \norm{w_j}_2$
		\STATE $q_{j + 1} := w_j / h_{j + 1, j}$
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Arnoldi(s,t)}
Arnoldi($s,t$) was first introduced by Hoemmen et al. in \cite{Hoemmen:2010:CKS:1970638} and is based on Walkers Householder Arnoldi method \cite{doi:10.1137/Walker} and the $s$-step Arnoldi method of Kim and Chronopoulos \cite{Chronopoulos}. Like Walkers version, Arnoldi($s,t$) produces $s$ basis vectors at once, except that, after $s$ steps, Walkers method must restart. This makes choosing a good $s$ difficult. If $s$ is too short, the method may converge too slow or may not converge at all. If $s$ is too large, the method is not numerically stable.\\ Arnoldi($s,t$) decouples the step size from the restart length by introducing an additional parameter $t$. The parameter $t$ refers to the number of iterations that produce $s$ orthonormal basis vectors before the method must restart. Therefore, the restart length $m$ is given by $m = s \cdot t$, where the basis length $s$ refers to the number of \textit{inner} iterations and $t$ denotes the number of \textit{outer} iterations.
The $s$-step Arnoldi method of Kim and Chronopoulos also shares this independence property, but is not as effective in terms of avoiding communication and basis orthogonalization. Also, while Arnoldi($s,t$) can use any $s$-step basis, Kim and Chronopoulos' algorithm is restricted to the Monomial basis only.\\
Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} use the MPK to produce  $s$ basis vectors at each outer iteration and considered the Monomial, the Newton and the Chebyshev basis. Depending on the basis type, their MPK has to compute either a one-term, two-term, or three-term recurrence respectively. Among the three, the Chebyshev basis is the most expensive to compute and also reacts more sensitive to bad eigenvalue approximations than the Newton basis (see Section~\ref{sec:Newtonbasis}). Therefore, it is not considered here. A more detailed description of the Chebyshev basis can be found, e.g. in \cite{Hoemmen:2010:CKS:1970638} and in Joubert and Carey \cite{Joubert_Carey_Chebyshev}.

\subsubsection{The Monomial basis}
The Monomial basis in s-step Krylov methods is given by 
\begin{equation*}
\mathcal{K}_{s + 1}(A, v) = [v, Av, A^2v, \ldots, A^sv]
\end{equation*}
and has a change of basis matrix
\begin{equation*}
\underline{B} = [\sigma_1 e_2, \sigma_2 e_3, \ldots, \sigma_s e_{s + 1}].
\end{equation*}
(where $\sigma_1, \ldots, \sigma_s$ are scaling factors) that satisfies
\begin{equation} \label{eq:AV=VB}
AV = \underline{V} \underline{B}
\end{equation}
with $V$ and $\underline{V}$ having dimensions $n \times s$ resp. $n \times s + 1$ and $\underline{B}$ being an $s + 1 \times s$ structurally upper Hessenberg matrix.
The Monomial basis is also known as the \textit{power method} which is an iterative method for finding the principal eigenvalue and corresponding eigenvector of a matrix by repeatedly applying a starting vector to the matrix. If the matrix and starting vector satisfy certain conditions, the basis converges to the principal eigenvector. Ideally, a basis has orthogonal basis vectors and should not converge. In theory, the converged basis is still linearly independent in exact arithmetic. In machine arithmetic, the basis vectors become inevitably dependent at some point. Therefore, other bases are considered, (e.g. Newton or Chebyshev) that provide better numerical stability.\\

The similarity between (\ref{eq:AV=VB}) and the Arnoldi relation (\ref{eq:AQ=QH}) can be used in order to reconstruct the upper Hessenberg matrix $\underline{H}$. The vectors created from the QR factorization of $\underline{V}$ might differ from the ones created by the standard Arnoldi method by a unitary scaling (see Section~\ref{sec:scaling_first_basis_vec} for details). For simplicity, it is assumed, that the QR factorization $\underline{V} = \underline{Q} \underline{R}$ produces the same unitary vectors as standard Arnoldi. From
\begin{eqnarray} \label{eq:unitary_scaling}
	AV &=& \underline{V} \underline{B} \nonumber\\
	AQR &=& \underline{Q} \underline{R} \underline{B} \nonumber\\
	AQ &=& \underline{Q} \underline{R} \underline{B} R^{-1}
\end{eqnarray}
emerges
\begin{equation} \label{eq:H=RBR}
	\underline{H} = \underline{R} \underline{B} R^{-1}.
\end{equation}
\\
Since $\underline{H}$ is upper Hessenberg, $\underline{B}$ must at least be structurally upper Hessenberg as well. This is in fact the case for the Monomial basis.

\subsubsection{The Newton basis} \label{sec:Newtonbasis}
The Newton basis in s-step Krylov methods is given by 
\begin{equation*}
\mathcal{K}_{s + 1}(A, v) = \left[v, (A - \theta_1 I )v, (A - \theta_2 I )(A - \theta_1 I )v, \ldots, \displaystyle\prod_{i = 1}^s (A - \theta_i I )v\right]
\end{equation*}
and has a change of basis matrix
\begin{equation}
\underline{B} = 
\begin{pmatrix}
\theta_1 & 0 & \ldots & 0 \\
\sigma_1 & \theta_2 & \ddots & \vdots \\
0 & \sigma_2 & \ddots & 0 \\
\vdots & \ddots & \ddots & \theta_s \\
0 & 0 & \ldots & \sigma_s 
\end{pmatrix}
\end{equation}
with scaling factors $\sigma_1, \ldots, \sigma_s$ and shifts $\theta_1, \ldots, \theta_s$.\\

Since $\underline{B}$ is structurally Hessenberg, Equation~\eqref{eq:H=RBR} holds for the Newton basis as well.\\

%Unlike the Monomial basis, the Newton basis uses a two-term recurrence $v_{i + 1} = Av_i - \theta_i v_i$.\\
Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638}, among many other authors, choose the shifts to be the eigenvalues of the upper Hessenberg matrix $H$ (the \textit{Ritz} values), because the Arnoldi iteration implicitly constructs an interpolating polynomial of the characteristic polynomial of $A$ at these points. Reichel \cite{Reichel1990} showed that, at least for normal matrices, the condition number of the Newton basis grows sub-exponentially in the number of interpolation points, whereas the Monomial basis has exponential growth. However, poor approximations of the eigenvalues of $A$ may lead to faster growth of the basis condition number than expected. For good approximations $A - \theta I$ may be ill-conditioned and several nearly identical shifts in a row could lead to an ill-conditioned basis. Therefore, the shifts have to be ordered in a way that makes the Newton basis as dissimilar from the Monomial basis as possible. This is accomplished by the Leja ordering.

%polinomial interpolation at shifts $\theta_1$, $\theta_2$, $\ldots$, $\theta_s$.

\paragraph{The Leja ordering}
This section only provides an overview of the Leja ordering. For a more detailed description see Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638}.
 The Leja ordering takes as input a set of shifts $\theta_1, \ldots, \theta_s$ and orders them so that a particular measure of the 'distance' of the current shift $\theta_j$ is maximized to the previous shifts $\theta_1, \ldots, \theta_{j - 1}$ (see ~\eqref{eq:prod_to_max}). These shifts may come in complex conjugate pairs and with some multiplicity $\mu$ (Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} point out, that the Ritz values from a Krylov method may be unique in exact arithmetic, but need not be in machine arithmetic). If the ordered Ritz values have consecutive complex conjugate pairs, the Newton basis can be computed using real arithmetic only (see the next paragraph for details). Unfortunately, the Leja ordering may separate complex conjugate pairs during the ordering process. The \textit{Modified} Leja ordering is an extension of the Leja ordering and ensures that complex conjugate pairs stay in consecutive order with the leading entry always comprising the positive imaginary part. The ordering process for both the Leja and Modified Leja orderings comprises the sets $K_j$ for $j = 1,2, \ldots, s$, where $K_j$ is given by
\begin{equation}
K_j := \{\theta_{j + 1}, \theta_{j + 2}, \ldots, \theta_s\}.
\end{equation}
The first shift $\theta_1$ is chosen by
\begin{equation}
\theta_1 = \text{argmax}_{z \in K_0} |z|.
\end{equation}
Subsequent shifts $\theta_2, \ldots, \theta_s$ are chosen using the rule
\begin{equation}\label{eq:prod_to_max}
\theta_{j + 1} = \text{argmax}_{z \in K_j} \prod_{k = 0}^j |z - z_k|^{\mu_j}
\end{equation}
where $\mu_j$ denotes the number of occurrences of $z_k$ (i.e., the multiplicity of $z_k$).
The product to maximize in Equation~\eqref{eq:prod_to_max} may underflow (for shifts that are close together) or overflow (for shifts that are far apart) in machine arithmetic. In order to prevent this, Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} suggest to scale the shifts with a capacity estimate. Scaling by a capacity estimate may still fail. In this case, Hoemmen et al. restart the computation with slightly randomly perturbed input values and repeat this process with growing perturbations until the method succeeds or an iteration limit has been reached.


\paragraph{Avoiding complex arithmetic} \label{sec:avoid_compl_arithm}
Like the eigenvalues of a real matrix, the Ritz values can occur as complex conjugate pairs. The Modified Leja ordering ensures that these pairs are ordered consecutively with leading positive imaginary entries, i.e. $\theta_{j + 1} := \overline{\theta}_j$ with $\Im(\theta_j) > 0$. Complex arithmetic doubles the storage and floating point operations and therefore, should be avoided. Instead of computing $v_{j + 1} = (A - \theta_j I)v_j$ and $v_{j + 2} = (A - \overline{\theta}_j I)v_{j + 1}$ like one would normally do, Bai et al. \cite{doi:10.1093/imanum/NewtonGMRES_bai} suggest that complex arithmetic can be skipped by setting
\begin{equation}
v_{j + 1} = (A - \Re(\theta_j) I )v_j
\end{equation}
and
\begin{equation}
v_{j + 2} = (A - \Re(\theta_j) I )v_{k + 1} + \Im(\theta_j)^2 v_j.
\end{equation}
It can easily be shown that 
\begin{eqnarray*}
v_{j + 2} &=& (A - \Re(\theta_j) I )^2v_{j} + \Im(\theta_j)^2 v_j \\
&=& (A - \overline{\theta}_j I )(A - \theta_j I )v_{j}.
\end{eqnarray*}

This also affects the change of basis matrix $\underline{B}$. If the Ritz values contain complex conjugate pairs, $B$ is tridiagonal. E.g., if $\theta_1$ through $\theta_s$ are real, with the exception of $\theta_j$ and $\theta_{j + 1}$ being a complex conjugate pair, the change of basis matrix is given by
\begin{equation*}
\iffalse
\begin{matrix}
\ddots & \ddots & \ddots & \ddots & \\
& \sigma_{j - 1} & $\scalebox{.7}[1]{$\Re$(}$ \theta_j$\scalebox{.7}[1]{$)$}$ & $\scalebox{.7}[1]{$-\Im($}$\theta_j$\scalebox{.7}[1]{$)$}$^2 & \\
& 0 & \sigma_j & $\scalebox{.7}[1]{$\Re$(}$ \theta_{j + 1}$\scalebox{.7}[1]{$)$}$ & \\
& & 0 & \sigma_{j + 1} & \\
& & \ddots & \ddots & \ddots \\
\end{matrix}
\fi
\underline{B} =
\begin{pmatrix}
\theta_1 & 0 & \ldots & \ldots & \ldots & 0 \\
\sigma_1 & \ddots & \ddots & \ddots & \ddots & \vdots \\
0 & \ddots & $\scalebox{.7}[1]{$\Re$(}$ \theta_j$\scalebox{.7}[1]{$)$}$ & $\scalebox{.7}[1]{$-\Im($}$\theta_j$\scalebox{.7}[1]{$)$}$^2 & \ddots & \vdots \\
\vdots & \ddots & \sigma_j & $\scalebox{.7}[1]{$\Re$(}$ \theta_{j + 1}$\scalebox{.7}[1]{$)$}$ & \ddots & \vdots \\
\vdots & \ddots & \ddots & \sigma_{j + 1} & \ddots & 0 \\
\vdots & \ddots & \ddots & \ddots & \ddots & \theta_s \\
0 & \ldots & \ldots & \ldots & 0 & \sigma_s \\
\end{pmatrix}.
\end{equation*}

\subparagraph{Performance notes}
Avoiding complex arithmetic in that way necessitates an extra SpMV operation in the Newton basis which, in the worst case, leads to as many floating point operations as in the Chebyshev basis (which uses a three-term recurrence). However, in their performance analysis Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} observed that the runtime of the Newton basis was still close to the runtime of the Monomial basis.\\
\cite{Hoemmen:2010:CKS:1970638} further point out, that this approach might lose accuracy when $\theta_{j - 1}$ is real and $\theta_j$ and $\theta_{j + 1}$ form a complex conjugate pair with $\Re(\theta_j) = \theta_{j - 1} $. Then
\begin{eqnarray*}
v_j &=& (A - \theta_{j - 1} I )v_{j - 1} \\
v_{j + 1} &=& (A - \Re(\theta_j) I )(A - \theta_{j - 1} I )v_{j - 1} \\
		  &=& (A - \theta_{j - 1} I )^2v_{j - 1}
\end{eqnarray*}
is equivalent to computing the Monomial basis with a possibly ill-conditioned  matrix $A - \theta_{j - 1} I$. This might occur if the Ritz values reside within an ellipse with a long vertical axis and very short horizontal axis on the complex plane.

\subsubsection{The Arnoldi(s,t) algorithm}

\begin{algorithm}
\caption{Arnoldi($s,t$)}
\label{alg:arnoldi(s,t)}
\begin{algorithmic}[1]
    \REQUIRE $n \times n$ matrix $A$ and starting vector $v$ of size $n$\\
    \ENSURE An orthonormal $n \times st + 1$ matrix $\underline{\frak{Q}} = \left[\frak{Q},q_{st + 1}\right]$ and a nonsingular $st + 1 \times st$ upper Hessenberg matrix $\underline{H}$ such that $AQ = \underline{Q} \underline{H}$
    \STATE $\beta := \norm{v}_2$, $q_1 := v/\beta$
    \FOR{$k = 0$ to $t - 1$}
    	\IF{$k = 0$}
    		\STATE Compute $\underline{Q}_0$ and $\underline{H}_0$ using Algorithm~\ref{alg:mgs_arnoldi}
    		\STATE Set $\underline{\frak{Q}}_0 := \underline{Q}_0$ and $\underline{\frak{H}}_0 := \underline{H}_0$
			\STATE Compute Ritz values from $H_0$
    	\ELSE
    		\STATE Fix basis conversion matrix $\underline{B}_k$
    		\STATE Compute $\underline{\acute{V}}_k$
    		\STATE $\underline{\acute{\frak{R}}}_{k - 1, k} := \underline{\frak{Q}}^T_{k - 1} \underline{\acute{V}}_k$
    	\STATE $\underline{\acute{V}}_k' := \underline{\acute{V}}_k - \underline{\frak{Q}}_{k - 1} \underline{\acute{\frak{R}}}_{k - 1, k}$
    	\STATE Compute QR factorization of $\underline{\acute{V}}'_k \rightarrow \underline{\acute{Q}}_k \underline{\acute{R}}_k$ using TSQR
    	\STATE Compute $\underline{\frak{H}}_{k - 1, k} := - \frak{H}_{k - 1} \frak{R}_{k - 1, k} R^{-1}_k + \underline{\frak{R}}_{k - 1, k} \underline{B}_k R^{-1}_k$
		\STATE Compute $H_k := R_k B_k R^{-1}_k + \tilde{\rho}^{-1}_k b_k z_k e^T_s - h_{k-1} e_1 e^T_{sk} \frak{R}_{k-1,k} R^{-1}_k$
		\STATE Compute $h_k := \tilde{\rho}^{-1}_k \rho_k b_k$
    	\STATE $\underline{\frak{H}}_k : = 
    	\begin{pmatrix}
    		\frak{H}_{k - 1} & \underline{\frak{H}}_{k - 1, k}\\
    		h_{k - 1} e_1 e_{sk}^T & H_k\\
    		0_{1,sk} & h_k e^T_s
    	\end{pmatrix}$
    	\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}

In this thesis, the main focus lies on the Newton-based CA-GMRES method. Therefore, only the Netwon-based Arnoldi($s,t$) algorithm is outlined in Algorithm~\ref{alg:arnoldi(s,t)} and described below.\\

\paragraph{First inner iteration} The Arnoldi($s,t$) algorithm starts its first outer iteration with $s$ steps of the standard Arnoldi method which results in an $n \times s + 1$ basis vector matrix $\underline{Q}_0$, with
\begin{equation}
\underline{Q}_0 = [Q_0, q_{s + 1}] = [q_1, q_2, \ldots, q_s, q_{s + 1}]
\end{equation}
and an $s + 1 \times s$ nonsingular upper Hessenberg matrix $\underline{H}_0$ that satisfies
\begin{equation}
AQ_0 = \underline{Q}_0 \underline{H}_0.
\end{equation}
Approximations of the eigenvalues of $A$ are obtained by computing the eigenvalues of the $s \times s$ upper submatrix of $\underline{H}_0$, namely $H_0$. As mentioned in Section~\ref{sec:Newtonbasis}, the eigenvalues of $H_0$ are also known as the Ritz values $\theta_1, \ldots, \theta_s$ and will be used as shifts for the Newton basis in successive outer iterations.\\

\paragraph{Successive inner iterations} At iteration $k$ (with $k > 0$) the last basis vector of the previous iteration is taken as the new start vector $v_{sk + 1}$ and the Newton basis vector matrix \begin{equation}
 \underline{\acute{V}}_k = [\acute{V}_k, v_{s(k + 1) + 1}] = [v_{sk+2}, \ldots, v_{s(k + 1)}, v_{s(k + 1) + 1}]
\end{equation}
is computed. Then, the upper Hessenberg matrix $\frak{H}_k$ must be recreated. From Relation~\eqref{eq:AV=VB} emerges
\begin{equation}
A [\underline{\frak{Q}}_{k - 1}, \acute{V}_k] = [\underline{\frak{Q}}_{k - 1}, \underline{\acute{V}}_k]\underline{\frak{B}}_k
\end{equation}
where $\underline{\frak{B}}_k$ satisfies
\begin{equation} \label{eq:B_blackletter_k_}
\underline{\frak{B}}_k = 
\begin{pmatrix}
	\frak{H}_{k - 1} & 0_{sk, s} \\
	h_{k - 1} e_1 e^T_{sk} & \underline{B}_k \\
\end{pmatrix}
\end{equation}
with $\frak{H}_0 := H_0$ and $
 \begin{pmatrix}
 \frak{H}_0\\
 h_{0} e^T_{sk} 
 \end{pmatrix}
:= \underline{H}_0
$.\\

Computing the QR factorization of $\acute{V}$ and $\underline{\acute{V}}$ respectively yields
\begin{equation*}
A [\underline{\frak{Q}}_{k - 1}, \acute{Q}_k] \cdot 
\frak{R}_k 
= [ \underline{\frak{Q}}_{k - 1}, \underline{\acute{Q}}_k ] \cdot 
\underline{\frak{R}}_k 
 \cdot
\underline{\frak{B}}_k
\end{equation*}
with
\begin{eqnarray*}
\frak{R}_k &=&
\begin{pmatrix}
	I_{sk + 1} & \acute{\frak{R}}_{k - 1, k}\\
	0_{s - 1, sk + 1} & \acute{R}_k\\
\end{pmatrix},\\
\underline{\frak{R}}_k &=&
\begin{pmatrix}
	I_{sk + 1} & \underline{\acute{\frak{R}}}_{k - 1, k}\\
	0_{s, sk + 1} & \underline{\acute{R}}_k\\
\end{pmatrix}
\end{eqnarray*}
where
\begin{eqnarray*}
	\frak{\acute{R}}_{k - 1, k} &=& \underline{\frak{Q}}_{k - 1}^T\acute{V}_k,\\
	\underline{\frak{\acute{R}}}_{k - 1, k} &=& \underline{\frak{Q}}_{k - 1}^T\underline{\acute{V}}_k
\end{eqnarray*}
are the interim results of the BCGS process and $\acute{R}_k$ and $\underline{\acute{R}}_k$ are the $R$ factors in the QR factorization of $V_k$ resp. $\underline{V}_k$.


The Arnoldi Relation~\eqref{eq:AQ=QH} yields
\begin{eqnarray*}
	A [\underline{\frak{Q}}_{k - 1}, \acute{Q}_k] = [ \underline{\frak{Q}}_{k - 1}, \underline{\acute{Q}}_k ] \underline{\frak{H}}_k
\end{eqnarray*}
and from the equation
\begin{eqnarray}
&A& [\underline{\frak{Q}}_{k - 1}, \acute{Q}_k] \cdot 
\begin{pmatrix}
	I_{sk + 1} & \acute{\frak{R}}_{k - 1, k} \\
	0_{s - 1, sk + 1} &\acute{R}_k \\
\end{pmatrix} \nonumber \\
&=& [ \underline{\frak{Q}}_{k - 1}, \underline{\acute{Q}}_k ] \cdot 
\begin{pmatrix}
	I_{sk + 1} & \underline{\acute{\frak{R}}}_{k - 1, k} \\
	0_{s, sk + 1} & \underline{\acute{R}}_k \\
\end{pmatrix} \cdot
\begin{pmatrix}
	\frak{H}_{k - 1} & 0_{sk, s} \\
	h_{k - 1} e_1 e^T_{sk} & \underline{B}_k \\
\end{pmatrix}
\end{eqnarray}
the upper Hessenberg matrix $\underline{\frak{H}}_k$ can be retrieved, with
\begin{eqnarray} \label{eq:AQR=QRB}
	\underline{\frak{H}}_k &=& \underline{\frak{R}}_k \underline{\frak{B}}_k \frak{R}^{-1}_k \nonumber\\
	&=& 
	\begin{pmatrix}
		I_{sk + 1} & \underline{\acute{\frak{R}}}_{k - 1, k} \\
		0_{s, sk + 1} & \underline{\acute{R}}_k \\
	\end{pmatrix} \cdot
	\begin{pmatrix}
		\frak{H}_{k - 1} & 0_{sk, s} \\
		h_{k - 1} e_1 e^T_{sk} & \underline{B}_k \\
	\end{pmatrix} \cdot
	\begin{pmatrix}
		I_{sk + 1} & \acute{\frak{R}}_{k - 1, k} \\
		0_{s - 1, sk + 1} &\acute{R}_k \\
	\end{pmatrix}^{-1}.
\end{eqnarray}

\paragraph{Updating the upper Hessenberg matrix}
Hoemmen et al. derive in \cite{Hoemmen:2010:CKS:1970638} how to compute Equation~\eqref{eq:AQR=QRB} efficiently. It suffices to show the results only here.
However, in order to understand their meaning, some matrices have to be broken down or must be repartitioned first. This has nothing to do with additional computations or data movement, it is purely notational.\\

First, four different parts of the basis vector matrix $V$ are defined by
\begin{eqnarray*}
\acute{V}_k &=& [v_{sk + 2}, \ldots, v_{s(k + 1)}],\\
\underline{\acute{V}}_k &=& [\acute{V}_k, v_{s(k + 1) + 1}],\\
V_k &=& [v_{sk + 1}, \acute{V}_k],\\
\underline{V}_k &=& [v_{sk + 1}, \underline{\acute{V}}_k].
\end{eqnarray*}
Then, four variations on the interim result of the BCGS process are defined by
\begin{eqnarray*}
	\frak{R}_{k - 1, k} &=& \frak{Q}_{k - 1}^TV_k,\\
	\underline{\frak{R}}_{k - 1, k} &=& \frak{Q}_{k - 1}^T\underline{V}_k,\\
	\frak{\acute{R}}_{k - 1, k} &=& \underline{\frak{Q}}_{k - 1}^T\acute{V}_k,\\
	\underline{\frak{\acute{R}}}_{k - 1, k} &=& \underline{\frak{Q}}_{k - 1}^T\underline{\acute{V}}_k.
\end{eqnarray*}
Given this notation, the $R$ factors $\underline{\frak{R}}_k$ and $\frak{R}_k$ can be repartitioned into
\begin{eqnarray*}
\underline{\frak{R}}_k &=&
\begin{pmatrix}
	I_{sk + 1} & \underline{\acute{\frak{R}}}_{k - 1, k} \\
	0_{s, sk + 1} & \underline{\acute{R}}_k \\
\end{pmatrix} = 
\begin{pmatrix}
	I_{sk} & \underline{\frak{R}}_{k - 1, k} \\
	0_{s + 1, sk} & \underline{R}_k \\
\end{pmatrix}, \\
\frak{R}_k &=&
\begin{pmatrix}
	I_{sk + 1} & \acute{\frak{R}}_{k - 1, k} \\
	0_{s - 1, sk + 1} & \acute{R}_k \\
\end{pmatrix} = 
\begin{pmatrix}
	I_{sk} & \frak{R}_{k - 1, k} \\
	0_{s, sk} & R_k \\
\end{pmatrix}
\end{eqnarray*}

where $R_k$ and $\underline{\acute{R}}_k$ are $s \times s$ matrices, $\underline{R}_k$ is $s + 1 \times s + 1$ and $\acute{R}_k$ is an $s - 1 \times s - 1$ matrix. (see Appendix~\ref{app:R_2} for a sketch of $\underline{\frak{R}}_2$ with $s = 3$)\\

Since the last basis vector of the previous iteration (e.g., the last column of $\underline{\frak{Q}}_{k - 1}$) is orthonormal to all previous basis vectors and since it is taken as the start vector of the current iteration, it holds that
\begin{equation*}
\underline{\frak{Q}}^T_{k - 1} \underline{V}_k e_1 = \underline{\frak{Q}}^T_{k - 1} q_{sk + 1} = e_{sk + 1}.
\end{equation*}

From that follows $\underline{\frak{R}}_{k - 1, k} e_1 = \frak{Q}^T_{k - 1} \underline{V}_k e_1 = \frak{Q}^T_{k - 1} q_{sk + 1} = 0_{sk, 1}$ resp. 
$\frak{R}_{k - 1, k} e_1 = 0_{sk, 1}$ and $\underline{R}_k e_1 = 1$ resp. $R_k e_1 = 1$.

$\underline{R}_k$ is broken down into
\begin{equation*}
\underline{R}_k = 
\begin{pmatrix}
	R_k & z_k\\
	0_{s, s} & \rho_k
\end{pmatrix}
\end{equation*}
where $z_k$ is $1 \times s$, $\rho_k$ is a scalar and $\tilde{\rho}^{-1}_k := R_k^{-1}(s,s)$. $\underline{B}_k$ is broken down into
\begin{equation*}
\underline{B}_k = 
\begin{pmatrix}
	B_k\\
	b \cdot e^T_s
\end{pmatrix}.
\end{equation*}
$\underline{\frak{H}}_k$ then consists of the following parts
\begin{equation}
\underline{\frak{H}}_k : = 
    	\begin{pmatrix}
    		\frak{H}_{k - 1} & \underline{\frak{H}}_{k - 1, k}\\
    		h_{k - 1} e_1 e_{sk}^T & H_k\\
    		0_{1,sk} & h_k e^T_s
    	\end{pmatrix}
\end{equation}
with
\begin{eqnarray} \label{eq:h_blackletter_k_}
\underline{\frak{H}}_{k - 1, k} &:=& -\frak{H}_{k - 1} \frak{R}_{k - 1, k} R^{-1}_k + \underline{\frak{R}}_{k - 1, k} \underline{B}_k R^{-1}_k,\\
H_k &:=& R_k B_k R^{-1}_k + \tilde{\rho}^{-1}_k b_k z_k e^T_s - h_{k-1} e_1 e^T_{sk} \frak{R}_{k-1,k} R^{-1}_k,\\
h_k &:=& \tilde{\rho}^{-1}_k \rho_k b_k.
\end{eqnarray}

\section{CA-GMRES} \label{sec:ca-gmres}
The CA-GMRES algorithm as in Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} uses all of Arnoldi($s,t$) together with a Givens rotation scheme that exposes the absolute residual error $\norm{b - Ax_k}_2$ at every inner iteration $j$. Therefore, the approximate solution $x_j$ need not be computed in order to estimate convergence. CA-GMRES also solves the linear least-squares problem 
\begin{equation*}
	y_k := argmin_y \norm{\underline{\frak{H}}_k y - \beta e_1}_2
\end{equation*}
at every outer iteration $k$ and then computes a new approximate solution $x_k := x_0 + \frak{Q}_k y_k$. The difference to GMRES($m$) is, that $s$ Givens rotations have to be applied at once. A Newton-based version of the CA-GMRES algorithm is outlined in Algorithm~\ref{alg:ca-gmres}.

\iffalse
The CA-GMRES algorithm solves a different least-squares problem than~\eqref{eq:stdgmreslsp}:
\begin{equation}
	\text{argmin}_y \norm{\beta e_1 - \underline{R} \underline{B} R^{-1} y}_2
\end{equation}
\fi

\begin{algorithm}
\caption{Newton-GMRES(s,t)}
\label{alg:ca-gmres}
\begin{algorithmic}[1]
    \REQUIRE $n \times n$ linear system $Ax = b$ and initial guess $x_0$
	\STATE restart := true
	\WHILE{restart}
    \STATE $r_0:=\textcolor{red}{b-Ax_0}$, $\beta := \norm{r_0}_2$, $q_1 := r_0/\beta$, 
    \FOR{$k = 0$ to $t-1$}
        \IF{$k = 0$}
        	\STATE Compute $\underline{Q}_0$ and $\underline{H}_0$ using MGS-Arnoldi
        	\STATE Set $\underline{\frak{Q}}_0 := \underline{Q}_0$ and $\underline{\frak{H}}_0 := \underline{H}_0$
        	\STATE Compute Ritz values from $H_0$
			\STATE Reduce $\underline{H}_0$ from upper Hessenberg to upper triangular form using $s$ Givens \\
\hspace{\algorithmicindent} rotations $G_1$, $G_2$, $ \ldots$, $G_s$. Apply the same rotations in the same order to \\
\hspace{\algorithmicindent} $\beta e_1$, resulting in the length $s + 1$ vector $\zeta_0$.
		\ELSE
			\STATE Fix basis conversion matrix $\underline{B}_k$
			\STATE Set $v_{sk + 1} := q_{sk + 1}$
			\STATE Compute $\underline{\acute{V}}_k$ where $v_{i + 1} = \textcolor{red}{(A - \theta_i I) v_i}$, for $i = sk + 1, \ldots, sk + s$
			\STATE $\underline{\acute{\frak{R}}}_{k-1,k} := \underline{\frak{Q}}^T_{k-1} \underline{\acute{V}}_k$
			\STATE $\underline{\acute{V}}'_k := \underline{\acute{V}}_k - \underline{\frak{Q}}_{k-1}\underline{\acute{\frak{R}}}_{k-1,k}$
			\STATE Compute QR factorization of $\underline{\acute{V}}'_k \rightarrow \underline{\acute{Q}}_k \underline{\acute{R}}_k$ using TSQR
			\STATE Compute $\underline{\frak{H}}_{k-1,1}:= - \frak{H}_{k-1} \frak{R}_{k-1,k} R^{-1}_k + \underline{\frak{R}}_{k-1,k} \underline{B}_k R^{-1}_k$
			\STATE Compute $H_k := R_k B_k R^{-1}_k + \tilde{\rho}^{-1}_k b_k z_k e^T_s - h_{k-1} e_1 e^T_{sk} \frak{R}_{k-1,k} R^{-1}_k$
			\STATE Compute $h_k := \tilde{\rho}^{-1}_k \rho_k b_k$
			\STATE $\underline{\frak{H}}_k := 
			\begin{pmatrix}
				\frak{H}_{k - 1} & \underline{\frak{H}}_{k - 1,k} \\
				h_{k - 1} e_1 e^T_{sk} & H_k \\
				0_{1,sk} & h_k e^T_s \\
			\end{pmatrix}$
			\STATE Apply Givens rotations $G_1$, $\ldots$, $G_{sk}$ in order to $\begin{pmatrix}
	\underline{\frak{H}}_{k - 1,k} \\
	\underline{H}_k \\
\end{pmatrix}$.
			\STATE Reduce $\underline{H}_k$ to upper triangular form using $s$ Givens rotations $G_{sk + 1}$, $\ldots$, \\
\hspace{\algorithmicindent} $G_{s(k + 1)}$. Apply the rotations in the same order to $\begin{pmatrix}
	\zeta_{k - 1} \\
	0_{s, 1}
\end{pmatrix}$, resulting in the \\
\hspace{\algorithmicindent} length \mbox{$s(k + 1) + 1$} vector $\zeta_k$.
		\ENDIF
	\STATE Element $s(k + 1) + 1$ of $\zeta_k$ is the 2-norm (in exact arithmetic) of the current \\
\hspace{\algorithmicindent} residual $r_{k + 1} = b - Ax_{k + 1}$ of the current solution $x_{k + 1}$.
	\IF{converged}
		\STATE restart = false, and exit for loop
	\ENDIF
	\ENDFOR
	\STATE Use the above reduction of $\underline{\frak{H}}_k$ to upper triangular form and $\zeta_k$ to solve $y_k :=$ \\ \hspace{\algorithmicindent} $\text{argmin}_{y} \norm{\underline{\frak{H}}_k y - \beta e_1}_2$
	\STATE Set $x_0 := x_0 + \frak{Q}_k y_k$
	\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Scaling the first basis vector} \label{sec:scaling_first_basis_vec}
The Monomial-based CA-GMRES method could also start without the standard Arnoldi method by generating an $n \times s + 1$ Monomial basis vector matrix first and then performing a QR factorization as in Equation~\eqref{eq:unitary_scaling}. The vectors created from the QR factorization might differ from the ones created by the standard Arnoldi method by a unitary scaling. Suppose that Arnoldi($s,t$) and the standard Arnoldi method start with the same starting vector $q_1$. After $st$ steps, Arnoldi($s,t$) developed an $n \times st + 1$ basis vector matrix $\underline{\frak{Q}}$ and an $st + 1 \times st$ upper Hessenberg matrix $\underline{\frak{H}}$ and standard Arnoldi produced an $n \times st + 1$ basis vector matrix $\underline{\hat{\frak{Q}}}$ and an $st + 1 \times st$ upper Hessenberg matrix $\underline{\hat{\frak{H}}}$. $\underline{\textfrak{Q}}$ then differs from $\underline{\hat{\textfrak{Q}}}$ by a unitary scaling $\underline{\Theta} := \text{diag}(\theta_1$, $\theta_2$, $\ldots$, $\theta_{st}$, $\theta_{st+1}$) with $\theta_j = |1|$,  $\forall j$, such that $\underline{\hat{\textfrak{Q}}} = \textfrak{\underline{Q}}\underline{\Theta}$. Also, $\underline{\frak{H}}$ differs from $\underline{\hat{\frak{H}}}$ such that $\hat{\underline{\frak{H}}} = \underline{\Theta}^T \underline{\frak{H}} \Theta$, where $\Theta$ is the $st \times st$ upper left submatrix of the $st + 1 \times st + 1$ diagonal matrix $\underline{\Theta}$. Suppose that iteration $st$ of standard GMRES computes an approximate solution $\textcolor{darkcyan}{\hat{x}} = x_0 + \color{darkcyan}\hat{\frak{Q}} \hat{y}$ and CA-GMRES computes $\textcolor{magenta}x = x_0 + \color{magenta}\frak{Q} y$. Hoemmen et al. show in \cite{Hoemmen:2010:CKS:1970638} that the computed solutions relate only through the first direction $\theta_1$, with $\textcolor{darkcyan}{\hat{x}} = x_0 + \theta_1 \color{magenta}\frak{Q}y$. I.e., if $\theta \ne 1$, the CA-GMRES solution $x$ will differ from the standard GMRES solution $\hat{x}$.
This issue can be addressed in multiple ways. Either the QR factorization used must not change direction of the first column or $\theta_1 = \left< r_0, q_1 \right>$ / $\beta$ must be computed which adds additional communication. A third approach is to compute the first set of basis vectors with the standard Arnoldi method (this happens naturally when the first outer iteration is started with standard Arnoldi in order to compute Ritz values for the Newton basis).

\subsection{A different approach}
If the Newton-based GMRES method in \cite{Erhel95aparallel} did not converge, Erhel computed 2$s$ Ritz values instead of $s$ for better eigenvalue approximations. She then picked $s$ of them and sorted them with the Modified Leja ordering. This improved the condition of the Newton basis and lead to (better) convergence in some cases. Hoemmen et al. $\cite{Hoemmen:2010:CKS:1970638}$ simply chose a smaller step size. This might give bad eigenvalue approximations and one might want to compute 2$s$ Ritz values anyway. However, these $2s$ values could come in complex conjugate pairs and have to amount to $s$ values eventually. If the $2s$ Ritz values only consist of complex conjugate pairs and $s$ is odd, a complex conjugate pair has to be split. To address this issue and still avoid complex arithmetic, one could either compute $2s + 1$ Ritz values or simply omit the positive imaginary part of the last Ritz value. Another approach might be to apply the 2$s$ Ritz values to two consecutive outer iterations of Arnoldi($s,t$). Since computing $2s$ Ritz values yields better approximations of the eigenvalues, one could incorporate them all as well. This slightly changes the way Equation~\eqref{eq:h_blackletter_k_} is computed. Remember that, in order to avoid complex arithmetic, the consecutive order of a complex conjugate pair must be preserved. In the case where $\theta_s$ is the first entry of a complex conjugate pair, the change of basis matrix $\underline{B}_{k - 1 }$ is connected to its consecutive change of basis matrix $\underline{B}_{k}$ by an additional entry right to the last Ritz value of $\underline{B}_{k - 1}$ and above the first Ritz value of $\underline{B}_k$. $\underline{\frak{B}}_k$ then differs from Equation~\eqref{eq:B_blackletter_k_} by that additional entry with
\begin{equation}
\underline{\frak{B}}_{k} = 
\begin{pmatrix}
	\frak{H}_{k - 1} & -e_{sk} e^T_1 \Im (\theta_s)^2 \\
	h_{k - 1} e_1 e^T_{sk} & \underline{B}_k
\end{pmatrix}.
\end{equation}

Equation~\eqref{eq:h_blackletter_k_} then changes to
\begin{equation}
\underline{\frak{H}}_{k - 1, k} := -\frak{H}_{k - 1} \frak{R}_{k - 1, k} R^{-1}_k + \underline{\frak{R}}_{k - 1, k} \underline{B}_k R^{-1}_k - \Im (\theta_s)^2 e_{sk} e^T_1  R^{-1}_{k}.
\end{equation}
However, this option might overcomplicate things without significant benefit.

\subsection{Preconditioning}
For better effectiveness GMRES is usually combined with a preconditioner. Different kinds of preconditioners exist, like left, right, and split preconditioners. In this thesis, only left preconditioners are considered. The preconditioner matrix $M^{-1}$ is applied to the left of $A$, where $M \approx A$. The system for CA-GMRES to solve is then given by $M^{-1}Ax = M^{-1}b$. Precisely, the preconditioner $M^{-1}$ must be applied to the red parts in Algorithm~\ref{alg:ca-gmres}, i.e. replace $r_0 = b - Ax_0$ by $r_0 = M^{-1}(b - Ax_0)$ and  $v_{i + 1} = (A - \theta_i)v_i$ by $v_{i + 1} = M^{-1}((A - \theta_i)v_i)$.\\

\textit{Scaling} is a special type of preconditioning. Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} considered two types of scaling in order to prevent rapid basis vector growth and improve numeric stability:
\begin{enumerate}
\item Balancing: replacing $A$ by $A' = DAD^{-1}$ with $D$ diagonal.
\item Equilibration: replacing $A$ by $A' = D_rAD_c$ with $D_r$ and $D_c$ diagonal, so that the largest absolute value in every row and every column is approximately equal to $1$.
\end{enumerate}
In their experiments for solving nonsymmetric linear systems with CA-GMRES Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} found that for practical problems, equilibration proved quite effective and almost made the basis type irrelevant. We observed mostly similar behavior after applying a standard version of the CA-ILU(0) preconditioner, which is described below.

\subsubsection{CA-ILU(0) preconditioner} \label{sec:ca-ilu}
The CA-ILU(0) is a communication-avoiding variant of the incomplete $LU$ factorization where the structure of $L$ resp. $U$ is the same as the lower resp. upper triangular part of $A$. The preconditioner $M = LU$ then approximates the matrix $A$ with $A = LU + R$, where $R$ is the residual matrix.\\
In addition to the matrix-vector multiplications in the MPK the CA-ILU(0) uses a forward and a backward substitution. Therefore, additional data has to be fetched and ghosted before the computations are done. This often leads to vast amounts of data transfer in the end. Grigori et al. \cite{Grigori} address this issue by first reordering the matrix $A$ via nested dissection and then reducing fills further by applying their novel Alternating min-max layers algorithm. While nested dissection affects convergence, Grigori et al. state that their novel algorithm does not. Also, the LU factorization itself can be done without communication because the graphs of $L$ and $U$ are known prior to their computation. 
\subsection{Convergence metrics}
CA-GMRES produces a cheap convergence metric, namely the absolute residual $\norm{r_{k+1}}_2$ relative to the initial residual $\norm{r_0}_2$. This metric might not be the best choice since this 'relative' residual error depends too strongly on the initial guess $x_0$ (e.g., if $\norm{x_0}_2$ is too large $\norm{r_0}$ will be large and the iteration will stop too early). Therefore, one might consider more expensive alternatives. For a fair comparison, the convergence metrics should be the same for both CA-GMRES and standard GMRES. Therefore, other metrics would only add the same amount of overhead. Since the methods are just compared here and since Hoemmen et al. use the cheap metric as well, both implemented methods do not incorporate other convergence metrics.

\subsection{Implementation details}\label{sec:implementation_details}
The CA-GMRES algorithm was implemented in the \texttt{C++} language and incorporates the LAPACK and BLAS routines provided by the Intel Math Kernel Library (MKL). The code layout was inspired by PETSc's KSP framework which is written in \texttt{C} but emulates object oriented design. Simple patterns like the Strategy Pattern were implemented as well.\\
BLAS and LAPACK functions are written in the Fortran language. Fortran itself stores matrices in column major order while C++ adopts the row major order layout. MKL provides a C-interface to most of the Fortran-style BLAS and LAPACK routines. The TSQR factorization, which is implemented in MKL as described in Section~\ref{sec:tsqr}, is one of the few routines that are used here but not supported. Therefore, dense matrices are stored in column major order and the unsupported Fortran routines are called directly.\\
MKL provides highly optimized routines that mostly benefit from cache-management techniques (e.g., \texttt{dgemm()}).
The major optimization techniques used in MKL include loop unrolling, blocking, and data prefetching.\\

All experiments and computations were done on the harris server. harris provides 4 sockets with 12 AMD Opteron(tm) cores per socket and one thread per core, leading to a total of 48 threads.

\paragraph{Differences to the implementation of Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638}}
\begin{itemize}
\item The CA-GMRES always starts its first inner iteration with the standard Arnoldi iteration, regardless of the basis type. This is not needed in case the Monomial basis is used. Starting with standard Arnoldi slows down the algorithm but might benefit the convergence if the Monomial basis is on the edge of being unstable.
\item The CA-GMRES always starts an outer iteration with the standard Arnoldi iteration. If the Newton basis is used, the first iteration must start with standard Arnoldi in order to compute Ritz values. If the Ritz values are good approximations to the eigenvalues, it might not be necessary to compute those values again in successive outer iterations. Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} do not address techniques or provide information on when to compute new Ritz values. One might monitor the estimate of the reciprocal condition number of the basis vector matrix and compute new values in case the condition number estimate worsens significantly. Since the $R$ factor of the QR factorization of $V$ and $V$ have the same condition number the complexity for computing the reciprocal condition number is rather cheap with $\mathcal{O}(s^2)$.
\item Although, the implemented Modified Leja ordering algorithm uses a capacity estimate to prevent over/underflows in the product to maximize, it does not ultimately perturb the input values if over/underflows still occur like in Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638}. Also, the algorithm depicted in \cite{Hoemmen:2010:CKS:1970638} and Appendix~\ref{app:MLO} has complexity $\mathcal{O}(s^3)$. Computing the Ritz values from an upper $s \times s$ Hessenberg matrix requires that many operations anyway. However, Bai et al. \cite{NewtonGMRES_bai_reichel/doi:10.1093/imanum} point out that the Modfied Leja ordering can be done in $\mathcal{O}(s^2)$ as well.
\item A method that computes 2$s$ Ritz values instead of $s$ (as in Erhel \cite{Erhel95aparallel}) was not implemented.
\item Neither the MPK nor the CA-ILU(0) preconditioner were fully implemented due to time constraints. Therefore, the CA-GMRES algorithm uses the highly optimized sparse matrix-vector routine that is provided by MKL.  Communication could be avoided anyway by giving MKL the hint that $s$ successive SpMV operations are coming up for the creation of the $s$ basis vectors. This optimization routine named \texttt{mkl\_sparse\_set\_mv\_hint()} would work for the Monomial basis, but might be ruined by the additional vector operation(s) needed in the Newton basis.
\item To avoid rapid growth in the length of the basis vectors the basis vectors may be scaled by their 2-norm right after they are generated. This would break the communication avoiding scheme of the MPK if it was implemented at this point. In order to avoid scaling and therefore, communication, Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} address this issue by equilibrating the matrix first.
\end{itemize}

\paragraph{how to build}
\begin{enumerate}
\item MKL and PAPI are required before building.
\item Adapt the Makefile in the source folder (e.g., change the MKL root directory, etc.).
\item type 'make -j8' to build.
\item type 'make test' for a small example.
\end{enumerate}


\paragraph{Problems and implementation difficulties}
\begin{itemize}
\item Memory leakage was observed, that originates from MKL. Valgrind reports that bytes are possibly lost and/or still reachable. This indicates that a library might not be freed somewhere. 
\item The ILU(0) preconditioner was implemented up to the point where all necessary values for the triangular solves and matrix-vector multiplications could be fetched. The permutation algorithm for minimizing fetching and ghosting was still missing as well as the actual triangular solving and matrix-vector multiplying routines.\\
The implemented sparse matrix manipulation routines include matrix (and vector) row and column permutations, row and column extractions, computing $A + A^T$ for a sparse matrix $A$ (needed by Metis, a hypergraph partitioner, which only works for undirected graphs) and a neighborhood method that uses the breadth first search algorithm and some reachability order to find the set of all reachable vertices from a subset of given vertices in a graph.
All these methods reside in the SparceUtils.cpp/.hpp files and have become mere artifacts.
\item CA-GMRES may fail for some matrices with the given true solution described in Section~\ref{sec:Numerical_experiments}, especially \texttt{Watt1}. In fact, finding a start vector that produces the results in Figures~\ref{fig:watt1_noscale} and \ref{fig:watt1_scale} was not easy. The issues for the \texttt{Watt1} matrix are known and can be addressed with the techniques described in \cite{Erhel95aparallel} and \cite{Hoemmen:2010:CKS:1970638}. Briefly those techniques comprise the computation of 2$s$ Ritz values instead of $s$, perturbing the input of the Modified Leja ordering and equilibrating the matrix before anything else. Equilibration could help with other matrices for which the CA-GMRES methods may fail as well.
\end{itemize}

\subsection{Test matrices}
For the numerical and performance experiments the following test matrices were used.
\begin{center}
\begin{tabular}{l|r|r|l}
Name & Dim & nnz & Type\\
\hline
watt1 & 1856 & 11550 & unsymmetric\\
sherman3 & 5005 & 20K & unsymmetric\\
dmat & 10K & 10K & diagonal\\
bcsstk18 & 12K & 149K & SPD\\
bmwst7\_1 & 141K & 3.7M & SPD\\
xenon2 & 157K & 3.9M & unsymmetric\\
pwtk & 218K & 3.7M & SPD
\end{tabular}
\end{center}

\section{Numerical experiments}\label{sec:Numerical_experiments}
This section provides a numerical comparison between CA-GMRES and standard GMRES. Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} showed in their numerical experiments, that for the correct choice of basis and parameters of the restart length $m = s \cdot t$ the CA-GMRES converges in no more iterations than standard GMRES (with the same restart length) for almost all test cases. This is due to the fact, that the ability to choose the $s$-step basis length way shorter than the restart length improves stability and performance. They suggest $s = 5$ and $t = 12$ often gave the best performance. In order to recreate their results, mainly the same parameter settings, matrices and right hand side vectors were used.\\

Like in \cite{Hoemmen:2010:CKS:1970638}, the true solution $\hat{x}$ is generated with
\begin{equation}
	\hat{x}(k) = u(k) + sin(2\pi k/n)
\end{equation}
where the scalar $u(k)$ is chosen from a random uniform [-1, 1] distribution. 
Hoemmen et al. state, that $\hat{x}$ was chosen in this way because a completely random solution is usually nonphysical, but a highly nonrandom solution (such as a vector of all ones) might be near an eigenvector of the matrix (which would result in artificially rapid convergence of the iterative method).

\subsection{Key for convergence plots}
The $x$ axis of the convergence plots shows the number of iterations of the Krylov methods tested. The $y$ axis of the convergence plots shows the 2-norm of the residual $\norm{b - Ax_k}_2$ relative to the 2-norm of the initial residual $\norm{r_0}_2$ at iteration $k$. Since standard GMRES exposes the residual at every iteration, it is presented as a continuous line. The $s$-step based CA-GMRES variants are depicted as points since their residuals are exposed only after every $s$ steps.\\

The terms in the legend are defined as follows.
\paragraph{GMRES(m)} Standard GMRES with restart length $m$ (the algorithm is outlined in Appendix~\ref{app:gmres(m)}). Its convergence metric is always shown as a black line.

\paragraph{Monomial-GMRES(s,t)} Monomial-based CA-GMRES with basis length $s$ and restart length $m = s \cdot t$. Its convergence metric is plotted as a circle shaped dot with different colors for both $s$ values shown in the plot.

\paragraph{Newton-GMRES(s,t)} Newton-based CA-GMRES with basis length $s$ and restart length $m = s \cdot t$. Its convergence metric is plotted as a triangle shaped dot with different colors for both $s$ values shown in the plot.\\

All plots show the standard GMRES, two versions of the Monomial-based CA-GMRES and two versions of the Newton-based CA-GMRES with different basis lengths. All methods have the same restart length, i.e., GMRES($m$) always uses $m = s \cdot t$ and $s_1 \cdot t_1$ is always equal to $s_2 \cdot t_2$ in the Newton-based resp. Monomial-based CA-GMRES versions.
The quality of the $s$-step basis for each method is shown as well. Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} compute the 2-norm condition number for the matrix $\underline{V}_k$ at iteration $k$, while an estimate for the reciprocal of the infinity-norm condition number $\mathcal{K}_{\infty}(\underline{\acute{V}}_k)$ is used here. Among all computed estimates for $\underline{\acute{V}}_k$ the minimum and maximum values are shown in the legend.

\begin{figure}[H]
	\centering
	\resizebox{.9\textwidth}{!}{\input{dmat3.tex}}	
	\caption{Convergence plot for a positive diagonal matrix with a 2-norm condition number $\mathcal{K}$ and logarithmically spaced eigenvalues between 1 and $1/\mathcal{K}$.}
	\label{fig:dmat3}
\end{figure}

\begin{figure}[H]
	\centering
	\resizebox{.9\textwidth}{!}{\input{watt1_noscale.tex}}
	\caption{Without basis vector scaling}
	\label{fig:watt1_noscale}
\end{figure}
\begin{figure}[H]
	\centering
	\resizebox{.9\textwidth}{!}{\input{watt1_scale.tex}}
	\caption{With basis vector scaling}
	\label{fig:watt1_scale}
\end{figure}
\begin{figure}[H]
	\centering
	\resizebox{.9\textwidth}{!}{\input{watt1_ilu0.tex}}
	\caption{Without ILU(0) preconditioner}
	\label{fig:watt1_ilu0}
\end{figure}
\begin{figure}[H]
	\centering
	\resizebox{.9\textwidth}{!}{\input{bmw7st1.tex}}
	\caption{Without basis vector scaling}
	\label{fig:bmw7st1}
\end{figure}

\begin{figure}[H]
	\centering
	\resizebox{.9\textwidth}{!}{\input{xenon2.tex}}
	\caption{Without basis vector scaling}
	\label{fig:xenon2_noscale}
\end{figure}
\begin{figure}[H]
	\centering
	\resizebox{.9\textwidth}{!}{\input{xenon2_scale.tex}}
	\caption{With basis vector scaling}
	\label{fig:xenon2_scale}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \resizebox{.9\textwidth}{!}{\input{sherman3.tex}}
  \caption{Without preconditioner}
  \label{fig:sherman3_unscaled}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \resizebox{.9\textwidth}{!}{\input{sherman3_ilu0.tex}}
  \caption{With ILU(0) preconditioner}
  \label{fig:sherman3_ilu0}
\end{subfigure}
\caption{A case where ILU(0) is in favor of the convergence behavior for all tested methods.}
\label{fig:sherman3_convergence}
\begin{subfigure}{.5\textwidth}
  \centering
  \resizebox{.9\textwidth}{!}{\input{xenon2.tex}}
  \caption{Without preconditioner}
  \label{fig:xenon2_noprec}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \resizebox{.9\textwidth}{!}{\input{xenon2_ilu0.tex}}
  \caption{With ILU(0) preconditioner}
  \label{fig:xenon2_ilu0}
\end{subfigure}
\caption{A case where ILU(0) adversely affects the convergence behavior for all tested methods.}
\label{fig:convergence_prec}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\resizebox{.9\textwidth}{!}{\input{bcsstk18_noscale.tex}}
	\caption{Without basis vector scaling}
	\label{fig:bcsstk18_noscale}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
	\centering
	\resizebox{.9\textwidth}{!}{\input{bcsstk18_scale.tex}}
	\caption{With basis vector scaling}
	\label{fig:bcsstk18_scale}
\end{subfigure}
\caption{Scaling the basis vectors does not help to stabilize the Newton-GMRES with the short basis length $s = 5$. It even significantly magnifies the convergence instability in the Newton-GMRES with longer basis length $s = 10$.}
\label{fig:bcsstk18_scaleissue}
\begin{subfigure}{.5\textwidth}
	\centering
	\resizebox{.9\textwidth}{!}{\input{pwtk_noscale.tex}}
	\caption{Without basis vector scaling}
	\label{fig:pwtk_noscale}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
	\centering
	\resizebox{.9\textwidth}{!}{\input{pwtk_scale.tex}}
	\caption{With basis vector scaling}
	\label{fig:pwtk_scale}
\end{subfigure}
\caption{In the unscaled case in figure~\ref{fig:pwtk_noscale}, the Newton-GMRES with the shorter basis length $s = 5$ is slightly unstable between 500 and 600 iterations. The scaling in figure~\ref{fig:pwtk_scale} fixes this, but drastically worsens the overall convergence for the Newton-GMRES with longer basis length $s = 10$.}
\label{fig:convergence_scaleissue}
\end{figure}

\subsection{Results for various matrices}
\subsubsection{Diagonal matrices}
CA-GMRES was tested with $10K \times 10K$ positive diagonal matrices (\texttt{Dmat}) with three different condition numbers $1\text{e}+05$, $1\text{e}+10$ and $1\text{e}+15$. Figure~\ref{fig:dmat3} shows how the Monomial basis fails to converge for larger $s$. Although the constructed matrices are the same as in Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} the convergence for the Monomial basis looks slightly different here because Hoemmen et al. do not start with standard Anoldi at every outer iteration. Interestingly, the Arnoldi residual converges further after the first inner iteration (it most definitely  diverges from the true residual at this point). Only the next restart cycle exposes the true and large residual in every third iteration. This phenomenon was observed by Greenbaum et al. \cite{Greenbaum97numericalbehaviour} when using a Householder-based GMRES instead of the MGS-based approach. The TSQR kernel in CA-GMRES uses Householder QR factorizations and therefore, might be the cause of this behavior.

\subsubsection{Matrices from applications}
\texttt{Watt1} is a small but very difficult matrix with two clusters of eigenvalues centered around one and zero. This impedes the computation of a good and linearly independent Newton basis. Heommen et al. used equilibration for numeric stability and could eventually converge with the same rate as standard GMRES by using a smaller step size as well. The results presented in Figures\ref{fig:watt1_noscale} and \ref{fig:watt1_scale} show similar problems for the unequilibrated case. Scaling the basis vectors by its 2-norm was helpful in this case and a larger step size could be chosen eventually. However the most effective way was to use the ILU(0) preconditioner (Figure~\ref{fig:watt1_ilu0}). Even the Monomial basis with a very large step size converged at the same rate as standard GMRES.\\

\texttt{Bmw7st\_1} (Figure~\ref{fig:bmw7st1}) is the next difficult matrix that Hoemmen et al. could only fix with equilibration. The matrix is badly scaled which made the equilibration process challenging as well. The \texttt{Bmw} matrix is another good example for how the CA-GMRES approach may fail. However, applying the ILU(0) preconditioner lets the method converge in one step for standard GMRES and in $s$ steps for CA-GMRES.\\

\texttt{Xenon2} is a good example for when the CA-GMRES works best. Figure~\ref{fig:xenon2_noscale} shows the convergence plot with unscaled basis vectors. Although the reciprocal condition number is extremely close to zero the convergence rate behaves rather well. This indicates that the vectors must be still linearly independent. Scaling the basis vectors as in Figure~\ref{fig:xenon2_scale} drastically improves the condition and leaves the convergence rate unchanged.\\

After Hoemmen et al. used equilibration on the \texttt{Xenon2} matrix, the convergence rate started to become unstable. Something similar can be observed with the ILU(0) preconditioner in Subfigure~\ref{fig:xenon2_ilu0}.\\
In general, Figure~\ref{fig:convergence_prec} shows two examples for when the ILU(0) improves the convergence rate and when it doesn't. Also, Figure~\ref{fig:convergence_scaleissue} discusses observed issues with basis vector scaling.



\section{Performance experiments}
This section describes performance experiments that compare the shared-memory implementation of CA-GMRES with the standard implementation GMRES($m$) on the harris platform.

\subsection{Key for speedup plots}
The left bars show the stacked relative runtimes for the kernels used in CA-GMRES. Those kernels comprise the first inner iteration (INIT), the sparse matrix-vector multiplications for creating the basis vectors (SpMV), the QR factorization of $V$ (TSQR), the orthogonalization against previous basis vectors (Block Gram-Schmidt) and the update of the upper Hessenberg Matrix (small dense operations). The right bars depict the stacked relative runtimes of the kernels used in GMRES($m$). These kernels include the sparse matrix-vector products (SpMV) and the modified Gram-Schmidt process (MGS). Solving the least squares problem at every outer iteration $k$ is not included since this kernel is the same for both methods. There is a difference on how the Givens rotations are applied. However, the cost for these Givens rotations is only marginal and therefore, negligible.\\
The numbers above the CA-GMRES bars indicate the speedup over standard GMRES. In some cases, the relative forward error was computed in order to verify the result of the computed solution. Here, the relative forward error is defined by
\begin{equation*}
	\frac{\norm{x - \hat{x}}_2}{\norm{\hat{x}}_2}
\end{equation*}
where $x$ is the approximation and $\hat{x}$ is the true solution for the system.

\begin{figure}[H]\label{runtime_threads_sherman3}
	\centering
	\resizebox{0.9\textwidth}{!}{\input{runtimes_threads_sherman3.tex}}
	\caption{This plot presents the runtimes for different numbers of threads for a small sparse preconditioned matrix (\texttt{Sherman3}). The runtimes for different threads are scaled by the runtime of the single threaded CA-GMRES. The tolerance $\epsilon$ for the convergence metric was set to $1\text{e}-14$. The convergence plot of this matrix is depicted in Figure~\ref{fig:sherman3_ilu0}. CA-GMRES converged at iteration 680, GMRES(m) converged at iteration 677. The relative forward error CA-GMRES is $6.61\text{e}-12$, the relative forward error for GMRES($m$) is $7.97\text{e}-12$. Computations were done 10 times and the average was taken.}
	\label{fig:runtimes_threads_sherman3}
\end{figure}

\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\input{runtimes_threads_xenon2.tex}}
	\caption{This plot presents the runtimes for different numbers of threads for a large sparse unpreconditioned matrix (\texttt{Xenon2}). The runtimes for different threads are scaled by the runtime of the single threaded CA-GMRES. The tolerance $\epsilon$ for the convergence metric was set to $5\text{e}-5$. This is too high for practical applications but it is needed in order to let the method converge in under 1000 iterations (see Figures~\ref{fig:xenon2_noscale},\ref{fig:xenon2_scale} for convergence plots). Both methods converged at iteration 390 and both methods have a very high relative forward error with $ 8.3951\text{e}+23$. The computations were done 10 times for each thread and the average runtime was taken.}
	\label{fig:runtimes_threads_xenon2}
\end{figure}
\begin{figure}[H]
	\centering
	\resizebox{1.0\textwidth}{!}{\input{runtimes_matrices.tex}}
	\caption{This plot shows the relative runtime for different matrices with optimal basis and restart lengths (not indicated) on 8 cores. The convergence metric was disabled and 1000 iterations were performed each. The matrices are presented in ascending order from left to right according to their size.}
	\label{fig:runtimes_matrices}
\end{figure}

\subsection{Results for various matrices}
Figure~\ref{fig:runtimes_threads_sherman3} shows an almost constant speedup factor of around $1.8$ between methods for different threads, but indicates speedups $< 1$ between same methods with increasing thread size. The dimension of the matrix used (\texttt{Sherman3}) is 5K and therefore, rather small. Using more than one thread is obviously not appropriate in this case.\\
Larger matrices like \texttt{Xenon2} with dimension 157K however do benefit from multiple threads. Figure~\ref{fig:runtimes_threads_xenon2} suggests around 32 to 40 threads would be optimal for a matrix of this size. Notice that, on the far right of the plot, more threads than available cores were used which further adds more overhead to the standard implementation rather than CA-GMRES.\\
Figure~\ref{fig:runtimes_matrices} shows the relative runtimes for three small matrices (\texttt{Watt1}, \texttt{Sherman3} and \texttt{Bcsstk18}) and two large ones (\texttt{Xenon2} and \texttt{Pwtk}). The different kernels in CA-GMRES are best visible in the smallest matrix, namely \texttt{Watt1}. However, even here the Block Gram-Schmidt process is too small to notice. This could be due to the fact that MKL is highly optimized for dense matrix-matrix operations. \texttt{Bcsstk18} has far more nonzeros compared to the other small matrices which might correlate with the significant speedup for CA-GMRES over the standard implementation.\\
The dominant kernel in CA-GMRES for large matrices is obviously SpMV. This is the only kernel that was not optimized (i.e., replaced with the matrix powers kernel from Heommen et al. \cite{Hoemmen:2010:CKS:1970638}). Also, as is discussed in Section~\ref{sec:implementation_details}, the INIT kernel may be reduced by shifting the work to the TSQR and 'small dense operations' kernels.

\section{Summary and Conclusion}
The numerical results show that CA-GMRES converges like standard GMRES under these conditions.
\begin{itemize}
\item The right basis length $s$ is chosen.
\item The right $s$-step basis is used.
\item Preconditioners (e.g. equilibration, etc.) are used.
\end{itemize}
Basis vector scaling was done in Bai et al. \cite{NewtonGMRES_bai_reichel/doi:10.1093/imanum} and Erhel \cite{Erhel95aparallel}.
The convergence plots show that scaling the basis vectors by its 2-norm may exacerbate convergence instabilities and, therefore, is not recommended. Hoemmen et al. \cite{Hoemmen:2010:CKS:1970638} use matrix equilibration instead.\\
The speedup plots clearly show the absence of the matrix powers kernel, especially for large matrices. Also, restarting with standard GMRES in every outer iteration of CA-GMRES may not be optimal and leaves room for optimization.

\bibliographystyle{plain}
\bibliography{BSc_Ernstbrunner_01403753}
 \listofalgorithms
 

\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}
\section{GMRES(m)}\label{app:gmres(m)}
\begin{algorithm}
\caption{GMRES(m)}
\label{alg:gmres(m)}
\begin{algorithmic}[1]
    \REQUIRE $n \times n$ linear system $Ax = b$ and initial guess $x_0$
    \STATE restart := true
    \WHILE{restart}
    	\STATE $r_0:=b - Ax_0$, $\beta := \norm{r_0}_2$, $q_0 := r_0/\beta$, $\underline{Q}_0 := q_0$, $\underline{H}_0 := \varnothing$
    	\FOR{$k = 1$ to $m$}
			\STATE Compute $q_k$ and $h_k$ using Algorithm~\ref{alg:mgs_arnoldi}
			\STATE Set $\underline{Q}_k := [Q_{k-1},q_k]$ and $\underline{H}_k := [\underline{H}_{k -1}, h_k]$
			\STATE Reduce ${h_k}$ of $\underline{H}_k$ from upper Hessenberg to upper triangular form using $k$ \\
		\hspace{\algorithmicindent} Givens rotations $G_1$, $G_2$, $ \ldots$, $G_{k}$. Apply the same rotations in the same order \\ 
		\hspace{\algorithmicindent} to $\beta e_1$,  resulting in the length $k + 1$ vector $\zeta_{k}$.
			\STATE Element $k + 1$ of $\zeta_k$ is the 2-norm (in exact arithmetic) of the current residual \\
\hspace{\algorithmicindent} $r_{k + 1} = b - Ax_{k + 1}$ of the current solution $x_{k + 1}$.
		\IF{converged}
			\STATE restart = false, and exit for loop		
		\ENDIF
		\ENDFOR
		\STATE Use the above reduction of $\underline{H}_k$ to upper triangular form and $\zeta_k$ to solve $y_k :=$ \\ \hspace{\algorithmicindent} $\text{argmin}_{y} \norm{\underline{H}_k y - \beta e_1}_2$
		\STATE Set $x_0 := x_0 + Q_k y_k$
	\ENDWHILE 
\end{algorithmic}
\end{algorithm}



\section{TODO}
\begin{itemize}
\item describe ILU(0)

\item write related work


\end{itemize}

\section{Visualizing the R matrix}\label{app:R_2}
\begin{center}
	\includegraphics[scale=0.5]{visualize_R2.png}
\end{center}
\section{Modified Leja Ordering}\label{app:MLO}
\begin{center}
	\includegraphics[scale=1.3]{ModifiedLejaOrdering.png}
\end{center}

\end{appendices}

\end{document}
% End of document.